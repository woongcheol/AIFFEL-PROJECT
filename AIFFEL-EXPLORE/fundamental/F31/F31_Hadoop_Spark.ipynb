{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alternative-martin",
   "metadata": {},
   "source": [
    "# F31. Hadoop and Spark\n",
    "## 1. PySpark 설치 및 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "planned-fetish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-cause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://wyvartccrutcsqjmr8k0ofq25-7458489bf9-cmfh8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext 모듈 import\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# SparkContext Configuration 세팅1\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "danish-spectrum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olympic-restriction",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-9fd4abc3ef2f>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d90d9efc10c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SparkContext 추가 생성 시 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-9fd4abc3ef2f>:4 "
     ]
    }
   ],
   "source": [
    "# SparkContext 추가 생성 시 에러 발생\n",
    "new_sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hindu-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bulgarian-balloon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://wyvartccrutcsqjmr8k0ofq25-7458489bf9-cmfh8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext Configuration 세팅2\n",
    "sc = SparkContext(master='local', appName='PySpark Basic')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "skilled-dating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.host', 'wyvartccrutcsqjmr8k0ofq25-7458489bf9-cmfh8'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1630627300278'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '44211'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext Configuration 확인\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "respective-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext Configuration 확인(단일)\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becoming-catalog",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext Configuration 확인(단일)\n",
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "speaking-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quantitative-seating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://wyvartccrutcsqjmr8k0ofq25-7458489bf9-cmfh8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext Configuration 세팅3\n",
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-effectiveness",
   "metadata": {},
   "source": [
    "## 2. RDD\n",
    "### 2-1. RDD Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "welcome-prince",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내부 데이터 집합 RDD 생성\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adverse-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "apparent-involvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actions\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "integrated-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "# 외부 데이터 로드 RDD 생성\n",
    "import os\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/test.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i)+'\\n')\n",
    "        \n",
    "print('완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instant-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/bigdata_ecosystem/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "emerging-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-defensive",
   "metadata": {},
   "source": [
    "### 2-2. RDD Operation(Transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dying-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "# map 예제 1\n",
    "x = sc.parallelize([\"b\", \"a\", \"c\", \"d\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect()) #collect()는 actions입니다. \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "outstanding-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "# map 예제 2\n",
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x: x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "incredible-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# filter 예제 1\n",
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.filter(lambda x: x%2 == 0) \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "short-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "# filter 예제 2\n",
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x: x.upper())\n",
    "A = capital.filter(lambda x: 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "inclusive-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# filtermap 예제 1\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "developing-desire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtermap 예제 1\n",
    "wordsDataset = sc.parallelize([\"Spark is funny\", \"It is beautiful\", \"And also It is implemented by python\"])\n",
    "result = wordsDataset.flatMap(lambda x: x.split()).filter(lambda x: x != \" \").map(lambda x: x.lower())\n",
    "\n",
    "# 공백 제거, 공백기준으로 split\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "matched-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 읽어오기\n",
    "import os\n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "brown-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 가공 - # 비어있는 라인 제외, delimeter(,)로 line 분리\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "increased-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼 분리\n",
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "operational-chrome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼부분 제외\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())  # 첫 번째 컬럼이 숫자인 것만 필터링\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efficient-president",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼 기준 정리\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "realistic-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 DataFrame 불러들이기\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "enhanced-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class |deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|0       |male  |66.0|0                 |0    |10.5   |Second|unknown|Southampton|y    |\n",
      "|0       |male  |42.0|1                 |0    |52.0   |First |unknown|Southampton|n    |\n",
      "|1       |female|49.0|1                 |0    |76.7292|First |D      |Cherbourg  |n    |\n",
      "|0       |male  |65.0|0                 |1    |61.9792|First |B      |Cherbourg  |n    |\n",
      "|0       |male  |45.0|1                 |0    |83.475 |First |C      |Southampton|n    |\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 40세 이상의 사람들 필터링\n",
    "df2 = df[df['age']>40]\n",
    "df2.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-aerospace",
   "metadata": {},
   "source": [
    "### 2-3. RDD Operation(Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "imperial-meaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceramic-duplicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "favorite-contrary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "lucky-victoria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "opponent-decade",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o364.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/aiffel/aiffel/bigdata_ecosystem/file.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-1be40d3bd8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/aiffel/bigdata_ecosystem/file.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -l ~/aiffel/bigdata_ecosystem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o364.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/aiffel/aiffel/bigdata_ecosystem/file.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/file.txt'\n",
    "nums.saveAsTextFile(file_path)\n",
    "\n",
    "!ls -l ~/aiffel/bigdata_ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "assumed-september",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 생성\n",
    "rdd = sc.parallelize(range(1,100))\n",
    "\n",
    "# RDD Transformation \n",
    "rdd2 = rdd.map(lambda x: 0.5*x - 10).filter(lambda x: x > 0)\n",
    "\n",
    "# RDD Action \n",
    "rdd2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-giant",
   "metadata": {},
   "source": [
    "### 2-4. RDD Operation(MapReduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "clean-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 2),\n",
       " ('e', 1),\n",
       " ('l', 2),\n",
       " ('o', 2),\n",
       " ('p', 1),\n",
       " ('y', 1),\n",
       " ('t', 1),\n",
       " ('n', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Word Counter 구현하기 문제\n",
    "text = sc.parallelize('hello python')\n",
    "\n",
    "# map 함수를 적용한 RDD 구하기\n",
    "text_1 = text.filter(lambda x: x != \" \")\n",
    "text_2 = text_1.map(lambda x:(x, 1))\n",
    "\n",
    "#reduceByKey 함수를 적용한 Word Counter 출력\n",
    "word_count = text_2.reduceByKey(lambda accum, n: accum + n)  \n",
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "wireless-blend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝 CSV 파일 로딩 내역 - 타이타닉\n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "columns = csv_data_1.take(1)\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "\n",
    "csv_data_3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "tribal-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생존자 평균 연령: 29.110411522633743\n",
      "사망자 평균 연령: 29.9609375\n"
     ]
    }
   ],
   "source": [
    "# 생존자와 사망자의 연령 총합 구하기\n",
    "csv_data_4 = csv_data_3.map(lambda line:(line[0][1], line[2][1]))   # (생존여부, 연령)\n",
    "age_sum_data = csv_data_4.reduceByKey(lambda accum, age: float(accum) + float(age))  \n",
    "age_sum = age_sum_data.collect()\n",
    "\n",
    "# 생존자와 사망자의 사람 수 구하기\n",
    "csv_data_5 = csv_data_3.map(lambda line:(line[0][1], 1))\n",
    "survived_data = csv_data_5.reduceByKey(lambda accum, count: int(accum) + int(count)) \n",
    "survived_count = survived_data.collect()\n",
    "\n",
    "age_sum_dict = dict(age_sum)\n",
    "survived_dict = dict(survived_count)\n",
    "avg_age_survived = age_sum_dict['1']/survived_dict['1']\n",
    "print('생존자 평균 연령:' ,avg_age_survived)\n",
    "avg_age_died = age_sum_dict['0']/survived_dict['0']\n",
    "print('사망자 평균 연령:' ,avg_age_died)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
