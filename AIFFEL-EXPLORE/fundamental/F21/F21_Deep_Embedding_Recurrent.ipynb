{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greatest-battlefield",
   "metadata": {},
   "source": [
    "# F20. DeepLearning : Linear, Convolution\n",
    "## 1. Linear Layer Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "editorial-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 1)\n",
      "\n",
      "2단계 연산 준비: (64, 4)\n",
      "2단계 연산 결과: (64,)\n",
      "2단계 Linear Layer의 Weight 형태: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 집약에 대한 행렬 변환(차원축소)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))     # Tensorflow는 Batch를 기반으로 동작하기에,\n",
    "                                         # 우리는 사각형 2개 세트를 batch_size개만큼\n",
    "                                         # 만든 후 처리를 하게 됩니다.\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "first_linear = tf.keras.layers.Dense(units=1, use_bias=False) \n",
    "# units은 출력 차원 수를 의미합니다.\n",
    "# Weight 행렬 속 실수를 인간의 뇌 속 하나의 뉴런 '유닛' 취급을 하는 거죠!\n",
    "\n",
    "first_out = first_linear(boxes)\n",
    "first_out = tf.squeeze(first_out, axis=-1) # (4, 1)을 (4,)로 변환해줍니다.\n",
    "                                           # (불필요한 차원 축소)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crazy-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4, 3)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 3)\n",
      "\n",
      "2단계 연산 준비: (64, 4, 3)\n",
      "2단계 연산 결과: (64, 4)\n",
      "2단계 Linear Layer의 Weight 형태: (3, 1)\n",
      "\n",
      "3단계 연산 준비: (64, 4)\n",
      "3단계 연산 결과: (64,)\n",
      "3단계 Linear Layer의 Weight 형태: (4, 1)\n",
      "총 Parameters: 13\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확장에 대한 변환(차원확장)\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))\n",
    "\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "first_linear = tf.keras.layers.Dense(units=3, use_bias=False)\n",
    "first_out = first_linear(boxes)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n3단계 연산 준비:\", second_out.shape)\n",
    "\n",
    "third_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "third_out = third_linear(second_out)\n",
    "third_out = tf.squeeze(third_out, axis=-1)\n",
    "\n",
    "print(\"3단계 연산 결과:\", third_out.shape)\n",
    "print(\"3단계 Linear Layer의 Weight 형태:\", third_linear.weights[0].shape)\n",
    "\n",
    "total_params = \\\n",
    "first_linear.count_params() + \\\n",
    "second_linear.count_params() + \\\n",
    "third_linear.count_params()\n",
    "\n",
    "print(\"총 Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-encyclopedia",
   "metadata": {},
   "source": [
    "## 2. Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aware-charge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 이미지 데이터: (64, 1920, 1080, 3)\n",
      "\n",
      "Convolution 결과: (64, 384, 216, 16)\n",
      "Convolution Layer의 Parameter 수: 1200\n",
      "\n",
      "1차원으로 펼친 데이터: (64, 1327104)\n",
      "\n",
      "Linear 결과: (64, 1)\n",
      "Linear Layer의 Parameter 수: 1327104\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer \n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "pic = tf.zeros((batch_size, 1920, 1080, 3))\n",
    "\n",
    "print(\"입력 이미지 데이터:\", pic.shape)\n",
    "conv_layer = tf.keras.layers.Conv2D(filters=16,\n",
    "                                    kernel_size=(5, 5),\n",
    "                                    strides=5,\n",
    "                                    use_bias=False)\n",
    "conv_out = conv_layer(pic)\n",
    "\n",
    "print(\"\\nConvolution 결과:\", conv_out.shape)\n",
    "print(\"Convolution Layer의 Parameter 수:\", conv_layer.count_params())\n",
    "\n",
    "flatten_out = tf.keras.layers.Flatten()(conv_out)\n",
    "print(\"\\n1차원으로 펼친 데이터:\", flatten_out.shape)\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "linear_out = linear_layer(flatten_out)\n",
    "\n",
    "print(\"\\nLinear 결과:\", linear_out.shape)\n",
    "print(\"Linear Layer의 Parameter 수:\", linear_layer.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-company",
   "metadata": {},
   "source": [
    "## 3. Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personalized-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 임포트 및 MNIST 데이터셋 로딩\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "import json\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "\n",
    "# MNIST 데이터 로딩\n",
    "(x_train, _), (x_test, _) = mnist.load_data()    # y_train, y_test는 사용하지 않습니다.\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impaired-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 4)           292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 4)           148       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 8)           296       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 3,369\n",
      "Trainable params: 3,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoder 모델 구성 - Input 부분\n",
    "input_shape = x_train.shape[1:]\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Encoder 부분\n",
    "encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_1 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_2 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_3 = MaxPooling2D((2, 2), padding='same')\n",
    "\n",
    "encoded = encode_conv_layer_1(input_img)\n",
    "encoded = encode_pool_layer_1(encoded)\n",
    "encoded = encode_conv_layer_2(encoded)\n",
    "encoded = encode_pool_layer_2(encoded)\n",
    "encoded = encode_conv_layer_3(encoded)\n",
    "encoded = encode_pool_layer_3(encoded)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Decoder 부분\n",
    "decode_conv_layer_1 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_1 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_2 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_3 = Conv2D(16, (3, 3), activation='relu')\n",
    "decode_upsample_layer_3 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_4 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "decoded = decode_conv_layer_1(encoded)   # Decoder는 Encoder의 출력을 입력으로 받습니다.\n",
    "decoded = decode_upsample_layer_1(decoded)\n",
    "decoded = decode_conv_layer_2(decoded)\n",
    "decoded = decode_upsample_layer_2(decoded)\n",
    "decoded = decode_conv_layer_3(decoded)\n",
    "decoded = decode_upsample_layer_3(decoded)\n",
    "decoded = decode_conv_layer_4(decoded)\n",
    "\n",
    "# AutoEncoder 모델 정의\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 19s 53ms/step - loss: 0.6860 - val_loss: 0.6847\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6842 - val_loss: 0.6825\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6818 - val_loss: 0.6796\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6787 - val_loss: 0.6759\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6746 - val_loss: 0.6707\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6690 - val_loss: 0.6636\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6611 - val_loss: 0.6529\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6491 - val_loss: 0.6362\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.6299 - val_loss: 0.6088\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.5989 - val_loss: 0.5669\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.5543 - val_loss: 0.5228\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.5136 - val_loss: 0.4987\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4930 - val_loss: 0.4875\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4825 - val_loss: 0.4795\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4749 - val_loss: 0.4727\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4678 - val_loss: 0.4668\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4623 - val_loss: 0.4614\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4570 - val_loss: 0.4564\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4519 - val_loss: 0.4516\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4470 - val_loss: 0.4470\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4424 - val_loss: 0.4424\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4380 - val_loss: 0.4378\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4332 - val_loss: 0.4332\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4291 - val_loss: 0.4286\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4244 - val_loss: 0.4238\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4196 - val_loss: 0.4191\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4150 - val_loss: 0.4143\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4105 - val_loss: 0.4096\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4051 - val_loss: 0.4048\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.4004 - val_loss: 0.3999\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3960 - val_loss: 0.3950\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3914 - val_loss: 0.3901\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3858 - val_loss: 0.3850\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3801 - val_loss: 0.3797\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3765 - val_loss: 0.3743\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3709 - val_loss: 0.3688\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3654 - val_loss: 0.3630\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3592 - val_loss: 0.3570\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3535 - val_loss: 0.3509\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3475 - val_loss: 0.3448\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3414 - val_loss: 0.3386\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3356 - val_loss: 0.3325\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3296 - val_loss: 0.3266\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3245 - val_loss: 0.3211\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3188 - val_loss: 0.3158\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3136 - val_loss: 0.3109\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3091 - val_loss: 0.3063\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3045 - val_loss: 0.3022\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.3010 - val_loss: 0.2984\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.2970 - val_loss: 0.2949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f16a67ad690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_10 = x_test[:10]       # 테스트 데이터셋에서 10개만 골라서\n",
    "x_test_hat = autoencoder.predict(x_test_10)    # AutoEncoder 모델의 이미지 복원생성\n",
    "x_test_imgs = x_test_10.reshape(-1, 28, 28)\n",
    "x_test_hat_imgs = x_test_hat.reshape(-1, 28, 28)\n",
    "\n",
    "plt.figure(figsize=(12,5))  # 이미지 사이즈 지정\n",
    "for i in range(10):  \n",
    "    # 원본이미지 출력\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(x_test_imgs[i])\n",
    "    # 생성된 이미지 출력\n",
    "    plt.subplot(2, 10, i+11)\n",
    "    plt.imshow(x_test_hat_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Conv2DTranspose\n",
    "\n",
    "# Conv2DTranspose를 활용한  AutoEncoder 모델\n",
    "# AutoEncoder 모델 구성 - Input 부분\n",
    "input_shape = x_train.shape[1:]\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Encoder 부분\n",
    "encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu')\n",
    "encode_pool_layer_1 = MaxPooling2D((2, 2))\n",
    "encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu')\n",
    "encode_pool_layer_2 = MaxPooling2D((2, 2))\n",
    "encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu')\n",
    "\n",
    "encoded = encode_conv_layer_1(input_img)\n",
    "encoded = encode_pool_layer_1(encoded)\n",
    "encoded = encode_conv_layer_2(encoded)\n",
    "encoded = encode_pool_layer_2(encoded)\n",
    "encoded = encode_conv_layer_3(encoded)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Decoder 부분  - \n",
    "decode_conv_layer_1 = Conv2DTranspose(4, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_1 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_2 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_2 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_3 = Conv2DTranspose(16, (3, 3), activation='relu')\n",
    "decode_upsample_layer_3 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_4 = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "decoded = decode_conv_layer_1(encoded)   # Decoder는 Encoder의 출력을 입력으로 받습니다.\n",
    "decoded = decode_upsample_layer_1(decoded)\n",
    "decoded = decode_conv_layer_2(decoded)\n",
    "decoded = decode_upsample_layer_2(decoded)\n",
    "decoded = decode_conv_layer_3(decoded)\n",
    "decoded = decode_upsample_layer_3(decoded)\n",
    "decoded = decode_conv_layer_4(decoded)\n",
    "\n",
    "# AutoEncoder 모델 정의\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
