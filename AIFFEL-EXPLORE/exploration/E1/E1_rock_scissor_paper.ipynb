{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890c1fb",
   "metadata": {},
   "source": [
    "# Project 1, Image Classfication : 가위바위보 판별\n",
    "## 1. 학습 및 테스트 데이터 준비\n",
    "### 1-1. 이미지 데이터 확보 및 사이즈 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bc1e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7817b7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Users\\\\woowo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HOMEPATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b87c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/scissor/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a4d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/rock/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3844c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dad98",
   "metadata": {},
   "source": [
    "### 1-2. 학습 데이터 적용 및 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35bd3c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcfa6167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f254bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af7f52",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 네트워크 구현\n",
    "### 2-1. 딥러닝 네트워크 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5da87d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a171abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f3515",
   "metadata": {},
   "source": [
    "### 2-2. 딥러닝 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edd90e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.1117 - accuracy: 0.2800\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0981 - accuracy: 0.3433\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.0997 - accuracy: 0.3400\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0840 - accuracy: 0.4200\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0674 - accuracy: 0.5767\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0346 - accuracy: 0.7200\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.9868 - accuracy: 0.7767\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.9108 - accuracy: 0.8400\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7990 - accuracy: 0.9267\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6465 - accuracy: 0.9267\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.5158 - accuracy: 0.8767\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4511 - accuracy: 0.8700\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.3284 - accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2483 - accuracy: 0.9533\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1991 - accuracy: 0.9767\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1577 - accuracy: 0.9800\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1158 - accuracy: 0.9967\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0963 - accuracy: 0.9967\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0915 - accuracy: 0.9900\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0693 - accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23629cdb9a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a8488",
   "metadata": {},
   "source": [
    "### 2-3. 테스트 데이터 적용 및 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ade369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e64f561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "주먹 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"주먹 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54d3d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b49081a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOMEPATH\") + \"/AIFFEL/rock_scissor_paper/test/\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f60ee87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd481042",
   "metadata": {},
   "source": [
    "### 2-4. 딥러닝 네트워크 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb291567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1314.0179 - accuracy: 0.4967\n",
      "test_loss: 1314.0179443359375 \n",
      "test_accuracy: 0.49666666984558105\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245ae47",
   "metadata": {},
   "source": [
    "### 2-5. 딥러닝 네트워크 성능 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67e38283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 15)        420       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 11, 11, 30)        4080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 30)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                37550     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 42,203\n",
      "Trainable params: 42,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.1165 - accuracy: 0.3167\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0904 - accuracy: 0.3300\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.0812 - accuracy: 0.3433\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1.0706 - accuracy: 0.6467\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1.0559 - accuracy: 0.7867\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1.0348 - accuracy: 0.8033\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0084 - accuracy: 0.7133\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.9569 - accuracy: 0.8167\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.8867 - accuracy: 0.8733\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.8051 - accuracy: 0.7967\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.9033\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.5821 - accuracy: 0.8967\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5082 - accuracy: 0.8167\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4116 - accuracy: 0.9133\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3363 - accuracy: 0.9367\n",
      "10/10 - 0s - loss: 448.3174 - accuracy: 0.6267\n",
      "test_loss: 448.3173828125 \n",
      "test_accuracy: 0.6266666650772095\n"
     ]
    }
   ],
   "source": [
    "# 바꿔 볼 수 있는 하이퍼파라미터들\n",
    "n_channel_1=15\n",
    "n_channel_2=30\n",
    "n_dense=50\n",
    "n_train_epoch=15\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)\n",
    "\n",
    "# 모델 시험\n",
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce64dc",
   "metadata": {},
   "source": [
    "# project summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb07ba",
   "metadata": {},
   "source": [
    "- project는 image classification project로, 총 300개의 이미지에 가위, 바위, 보 클래스를 labeling하였고 300개의 test data로 성능을 평가하였다.(300개는 직접 촬영, 300개는 다른 분의 손)\n",
    "- project에 딥러닝 모델은 tensorflow sequential 모델을 사용했으며 7개의 layer를 적용하여 구현했다. 이에 accuracy는 하이퍼파라미터 조정으로 0.3 - 0.6 분포를 보였으나 주로 0.3 - 0.4에서 평가되었다.\n",
    "- project를 마치며 학습 데이터의 양, 데이터의 품질, 모델 구조 그리고 하이퍼 파라미터등의 조정으로 accuracy를 높일 수 있음을 확인했으며 특히, 다양한 데이터의 양과 높은 품질이 크게 작용될 수 있음을 배우게 되었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
